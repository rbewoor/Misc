{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-dakota",
   "metadata": {},
   "source": [
    "# Following tutorial video as per info below\n",
    "\n",
    "Youtube video by \"freeCodeCamp.Org\" : PyTorch for Deep Learning - Full Course / Tutorial\n",
    "30 April 2020\n",
    "https://www.youtube.com/watch?v=GIsg-ZUy0MY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-fleet",
   "metadata": {},
   "source": [
    "# PyTorch basics - tensors and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amino-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-decision",
   "metadata": {},
   "source": [
    "## Defining different types of tensors - shape, dtype\n",
    "\n",
    "All elements in a tensor will have same data type.\n",
    "Tensor must have a regular shape, unlike lists.\n",
    "Tensor are suitable data type to execute on GPU. Numpy arrays execute on CPU.\n",
    "\n",
    "E.g. List can be = [[1, 2], [1, 2, 3]]. But this cannot be done for a tensor\n",
    "\n",
    "Dtypes expalined here: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latin-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.Size([])\n",
      "torch.float32\n",
      "--------\n",
      "tensor([1., 2., 3., 4.])\n",
      "torch.Size([4])\n",
      "torch.float32\n",
      "--------\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [11., 22., 33., 44.]])\n",
      "torch.Size([2, 4])\n",
      "torch.float32\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [11., 22., 33., 44.]], dtype=torch.float64)\n",
      "torch.Size([2, 4])\n",
      "torch.float64\n",
      "--------\n",
      "tensor([[[  1.,   2.,   3.,   4.],\n",
      "         [ 11.,  22.,  33.,  44.],\n",
      "         [111., 222., 333., 444.]],\n",
      "\n",
      "        [[  5.,   6.,   7.,   8.],\n",
      "         [ 55.,  66.,  77.,  77.],\n",
      "         [555., 666., 777., 888.]]])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# number\n",
    "t1 = torch.tensor(4.)\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "print(t1.dtype)\n",
    "print(\"--------\")\n",
    "# vector\n",
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "print(t2)\n",
    "print(t2.shape)\n",
    "print(t2.dtype)\n",
    "print(\"--------\")\n",
    "# 2d array\n",
    "t3 = torch.tensor([[1., 2, 3, 4], [11., 22, 33, 44]])\n",
    "print(t3)\n",
    "print(t3.shape)\n",
    "print(t3.dtype)\n",
    "# 2d array - changing the default dtype from float32 to float64\n",
    "t3_1 = torch.tensor([[1., 2, 3, 4], [11., 22, 33, 44]], dtype=torch.float64)\n",
    "print(t3_1)\n",
    "print(t3_1.shape)\n",
    "print(t3_1.dtype)\n",
    "print(\"--------\")\n",
    "# 3d array\n",
    "t4 = torch.tensor([\n",
    "    [[1., 2, 3, 4], [11, 22, 33, 44], [111, 222, 333, 444]],\n",
    "    [[5., 6, 7, 8], [55, 66, 77, 77], [555, 666, 777, 888]]\n",
    "])\n",
    "print(t4)\n",
    "print(t4.shape)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-employer",
   "metadata": {},
   "source": [
    "# Tensor operations and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quiet-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create three tensors, but two of them requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "serious-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "square-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = w * x + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "viral-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = None\n",
      "dy/dw = 3.0\n",
      "dy/db = 1.0\n"
     ]
    }
   ],
   "source": [
    "## now y is a tensor as expected. But, we can automatically compute the derivate\n",
    "## (aka gradient) of y wrt. the tensors that are defined with requires_grad=True.\n",
    "\n",
    "## But note there is a computation cost, so be careful defining with requires_grad=True\n",
    "\n",
    "## To compute these gradients, we simply call y.backward\n",
    "## The gradients are accessed using the .grad method on the tensors.\n",
    "\n",
    "y.backward()\n",
    "\n",
    "## See the gradients\n",
    "## Note the grad for x is None, as it is defined with requires_grad=False (default)\n",
    "\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "print(f\"dy/dw = {w.grad}\")\n",
    "print(f\"dy/db = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporated-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What if one or more of x, w or b were matrices and not just numbers?\n",
    "#    What would result y and the gradiets w.grad and b.grad look like?\n",
    "# 2. What if y was a matrix created using torch.tensor, with each element of the\n",
    "#    matrix expressed as a combination of numberic tensors x, w and b?\n",
    "# 3. What if we had a chain of operations of just one i.e.\n",
    "#    y = x * w + b\n",
    "#    z = 1 * y + m\n",
    "#    w = c * z + d\n",
    "#    What would calling w.grad do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seventh-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor(4., requires_grad=True)\n",
      "tensor(5., requires_grad=True)\n",
      "------\n",
      "tensor([[ 5.,  9., 13.],\n",
      "        [17., 21., 25.]], grad_fn=<AddBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8a148efb85ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "## 1st part - only x is a matrix\n",
    "x = torch.tensor(np.arange(6, dtype=np.float32).reshape(2,3))\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "print(f\"------\")\n",
    "y = w * x + b\n",
    "print(y)\n",
    "print(type(y))\n",
    "print(f\"------\")\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "meaningful-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]], dtype=torch.float64, requires_grad=True)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], dtype=torch.float64, requires_grad=True)\n",
      "------\n",
      "tensor([[ 5.,  9., 13.],\n",
      "        [17., 21., 25.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00feadbb63a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "## 1st part - x, w and b are matrices of same size and shape\n",
    "x = torch.tensor(np.arange(6, dtype=np.float32).reshape(2,3))\n",
    "#w = torch.tensor(np.array([4.] * 6).reshape(2,3), requires_grad=True)\n",
    "w = torch.tensor(np.full((2,3), 4.), requires_grad=True)\n",
    "b = torch.tensor(np.full((2,3), 5.), requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "print(f\"------\")\n",
    "y = w * x + b\n",
    "print(y)\n",
    "print(type(y))\n",
    "print(f\"------\")\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civic-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]], requires_grad=True)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], requires_grad=True)\n",
      "torch.Size([2, 3]) torch.Size([2, 3]) torch.Size([2, 3])\n",
      "------\n",
      "tensor([[ 5.,  9., 13.],\n",
      "        [17., 21., 25.]])\n",
      "<class 'torch.Tensor'>\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-829ee32f997d>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(w * x + b)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-829ee32f997d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv3rabbitai1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "## 2nd part - y created using torch.tensor\n",
    "x = torch.tensor(np.arange(6, dtype=np.float32).reshape(2,3))\n",
    "w = torch.tensor(np.full((2,3), 4., dtype=np.float32), requires_grad=True)\n",
    "b = torch.tensor(np.full((2,3), 5., dtype=np.float32), requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "print(x.shape, w.shape, b.shape)\n",
    "print(f\"------\")\n",
    "y = torch.tensor(w * x + b)\n",
    "print(y)\n",
    "print(type(y))\n",
    "print(f\"------\")\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affected-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "tensor(17., grad_fn=<AddBackward0>)\n",
      "<class 'torch.Tensor'>\n",
      "------\n",
      "x.grad = None\n",
      "w.grad = None\n",
      "b.grad = 1.0\n",
      "m.grad = None\n",
      "c.grad = None\n",
      "d.grad = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-660fdfc58779>:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(f\"{varname}.grad = {t.grad}\")\n"
     ]
    }
   ],
   "source": [
    "## 3rd part\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "m = torch.tensor(6., requires_grad=True)\n",
    "c = torch.tensor(7., requires_grad=True)\n",
    "d = torch.tensor(8., requires_grad=True)\n",
    "print(f\"------\")\n",
    "y = w * x + b\n",
    "z = 1 * y + m\n",
    "w = c * z + d\n",
    "print(y)\n",
    "print(type(y))\n",
    "print(f\"------\")\n",
    "y.backward()\n",
    "tensorlist = [x, w, b, m, c, d]\n",
    "for varname, t in zip([v for v in 'xwbmcd'], tensorlist):\n",
    "    print(f\"{varname}.grad = {t.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-airport",
   "metadata": {},
   "source": [
    "# Interoperability with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exclusive-fever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3.]\n",
      " [4. 5. 6. 7.]]\n",
      "(2, 4)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "#x = np.array([[1.,2],[3,4]])\n",
    "x = np.arange(8, dtype=np.float64).reshape(2,4)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "introductory-runner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float64)\n",
      "torch.Size([2, 4])\n",
      "torch.float64\n",
      "140279006117488 140279006156160\n"
     ]
    }
   ],
   "source": [
    "## convert np array to tensor using .from_numpy\n",
    "\n",
    "y_4mNp = torch.from_numpy(x)\n",
    "print(y_4mNp)\n",
    "print(y_4mNp.shape)\n",
    "print(y_4mNp.dtype)\n",
    "print(id(x), id(y_4mNp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "secondary-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float64)\n",
      "torch.Size([2, 4])\n",
      "torch.float64\n",
      "140279006117488 140279006173824\n"
     ]
    }
   ],
   "source": [
    "## convert np array to tensor using torch.tensor\n",
    "\n",
    "y_t = torch.tensor(x)\n",
    "print(y_t)\n",
    "print(y_t.shape)\n",
    "print(y_t.dtype)\n",
    "print(id(x), id(y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjusted-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note the differene between torch.tensor vs torch.from_numpy\n",
    "## From_numpy uses the same underlying memory that the np variable uses.\n",
    "##     So changing either the np or the .from_numpy variables impact each other but NOT\n",
    "##        the tensor variable.\n",
    "##     But changes to y_t affect only itself and not the numpy or the from_numpy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arranged-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[[0. 1. 2. 3.]\n",
      " [4. 5. 6. 7.]]\n",
      "y_4mNp=tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float64)\n",
      "y_t=tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x={x}\\ny_4mNp={y_4mNp}\\ny_t={y_t}\")  ## all have same values right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instant-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[[  0. 111.   2.   3.]\n",
      " [500. 500. 500. 500.]]\n",
      "y_4mNp=tensor([[  0., 111.,   2.,   3.],\n",
      "        [500., 500., 500., 500.]], dtype=torch.float64)\n",
      "y_t=tensor([[999., 999., 999., 999.],\n",
      "        [  4.,   5.,   6.,   7.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x[0,1] = 111       ## changed the numpy variable itself directly\n",
    "y_4mNp[1,:] = 500  ## changed the .from_numpy variable\n",
    "y_t[0,:] = 999     ## changed the tensor variable\n",
    "print(f\"x={x}\\ny_4mNp={y_4mNp}\\ny_t={y_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-tanzania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "statewide-outline",
   "metadata": {},
   "source": [
    "# Linear Regression with PyTorch\n",
    "\n",
    "Code without built in methods of pytorch.\n",
    "Explicitly updating the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "checked-moore",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-d750484eaae9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-d750484eaae9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    We are trying to make such a model\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "We are trying to make such a model\n",
    "\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "\n",
    "Essentially this would be a matrix multiplication of the form\n",
    "\n",
    "Y = X * transpose(W) + B\n",
    "\n",
    "Y is target values.\n",
    "X is the input values.\n",
    "W is the weights matrix.\n",
    "B is the biases matrix.\n",
    "\n",
    "First define the model, data and the loss functions.\n",
    "So the idea is:\n",
    "1. Generate predictions\n",
    "2. Calculate the loss e.g. with MSE\n",
    "3. Compute gradient using .backward().\n",
    "4. Use the .grad values of the variables to update their values as per the LR.\n",
    "5. Zero the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caring-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "received-block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) (5, 2)\n",
      "torch.Size([5, 3]) torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Numpy arrays first\n",
    "# Input (temp, rain, humid)\n",
    "inputs = np.array([\n",
    "    [73, 67, 43],\n",
    "    [91, 88, 64],\n",
    "    [87, 134, 58],\n",
    "    [102, 43, 37],\n",
    "    [69, 96, 70]\n",
    "], dtype='float32')\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([\n",
    "    [56, 70],\n",
    "    [81, 101],\n",
    "    [119, 133],\n",
    "    [22, 37],\n",
    "    [103, 119]\n",
    "], dtype='float32')\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "passive-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4372,  1.2163,  0.1074],\n",
      "        [ 0.8596, -1.3096, -0.3948]], requires_grad=True)\n",
      "tensor([-0.2865, -0.1088], requires_grad=True)\n",
      "torch.Size([2, 3]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "# Initialise randomly using the torch.randn function. This outputs values samples from a\n",
    "#    gaussian distribution with mean 0 and SD=1. So mostly the values will be in range\n",
    "#    -1 to +1 but some will be outside this range also.\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)\n",
    "print(w.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "stupid-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ is for matrix multiplication  and the .t() method to transpose\n",
    "def regr_model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cloudy-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  53.9129,  -42.0830],\n",
      "        [  73.8422,  -62.4041],\n",
      "        [ 130.8981, -123.7165],\n",
      "        [  11.3986,   16.6442],\n",
      "        [  93.8350,  -94.1602]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions       ##  (5,3) @ (2,3).t() + (2)\n",
    "preds = regr_model(inputs)   ##  (5,3) @ (3,2) + (5,2) = (5,2)\n",
    "print(preds)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cleared-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(p, t):\n",
    "    diff = p-t\n",
    "    return torch.sum(torch.square(diff)) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ideal-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15141.2012, grad_fn=<DivBackward0>)\n",
      "tensor(123.0496, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "## Suppose the loss = 3600 => on average the predictions are off from targets by a value of\n",
    "##     square_root 3600 = 60\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "print(torch.sqrt(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "spoken-profile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -296.4617,   -102.2158,   -178.3113],\n",
      "        [-12434.1016, -15525.5625,  -9168.2734]])\n",
      "tensor([  -3.4226, -153.1439])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "# .backward() method automatically does the gradient calculations and stores the values\n",
    "#     for each of the variables that have requires_grad=True.\n",
    "#     These gradients can be accessed using the .grad() method on those variables.\n",
    "loss.backward()\n",
    "\n",
    "# See the weights grads\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "abandoned-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the grads to 0 by calling the .zero_() method.\n",
    "# This is done as pytorch accumulates the gradients automatically and thus each time\n",
    "# .backward() is invoked on the loss, the new gradient values will be added to the old\n",
    "# existing values - which is not what we want to do.\n",
    "\n",
    "\n",
    "# Keep repeating above steps as per number of desired epochs.\n",
    "#w.grad.zero_()\n",
    "#b.grad.zero_()\n",
    "#print(w.grad)\n",
    "#print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad tells pytorch that we should not modify the gradients while updating the\n",
    "# weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "quarterly-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values BEFORE updating the weights and biases:\n",
      "tensor([[-0.4372,  1.2163,  0.1074],\n",
      "        [ 0.8596, -1.3096, -0.3948]], requires_grad=True)\n",
      "tensor([-0.2865, -0.1088], requires_grad=True)\n",
      "\n",
      "Values AFTER updating the weights and biases:\n",
      "tensor([[-0.4342,  1.2174,  0.1092],\n",
      "        [ 0.9839, -1.1544, -0.3031]], requires_grad=True)\n",
      "tensor([-0.2864, -0.1073], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Values BEFORE updating the weights and biases:\")\n",
    "print(w)\n",
    "print(b)\n",
    "# Adjust the weights and biases while resetting the gradients\n",
    "LR = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * LR\n",
    "    b -= b.grad * LR\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "print(f\"\\nValues AFTER updating the weights and biases:\")\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "exotic-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10770.3574, grad_fn=<DivBackward0>)\n",
      "tensor(103.7803, grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "# check new loss - it should have falled from earlier\n",
    "preds = regr_model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "print(torch.sqrt(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "metric-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mulitple epochs\n",
    "epochs = 300\n",
    "LR = 1e-4\n",
    "for i in range(epochs):\n",
    "    preds = regr_model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * LR\n",
    "        b -= b.grad * LR\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "personal-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6589, grad_fn=<DivBackward0>)\n",
      "tensor(0.8117, grad_fn=<SqrtBackward>)\n",
      "tensor([[ 57.1494,  70.3239],\n",
      "        [ 81.9703, 100.4016],\n",
      "        [119.2556, 133.5452],\n",
      "        [ 21.2429,  37.1811],\n",
      "        [101.3587, 118.5520]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[ 1.1494,  0.3239],\n",
      "        [ 0.9703, -0.5984],\n",
      "        [ 0.2556,  0.5452],\n",
      "        [-0.7571,  0.1811],\n",
      "        [-1.6413, -0.4480]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check new loss - it should have falled from earlier\n",
    "preds = regr_model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "print(torch.sqrt(loss))\n",
    "print(preds)\n",
    "print(targets)\n",
    "print(preds-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-depth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with PyTorch using PyTorch built-ins\n",
    "\n",
    "Using the optimizer and torch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays first\n",
    "# Input (temp, rain, humid)\n",
    "inputs = np.array([\n",
    "    [73, 67, 43],\n",
    "    [91, 88, 64],\n",
    "    [87, 134, 58],\n",
    "    [102, 43, 37],\n",
    "    [69, 96, 70]\n",
    "], dtype='float32')\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([\n",
    "    [56, 70],\n",
    "    [81, 101],\n",
    "    [119, 133],\n",
    "    [22, 37],\n",
    "    [103, 119]\n",
    "], dtype='float32')\n",
    "print(inputs.shape, targets.shape)\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-courtesy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-banana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-calvin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-collins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-universal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
