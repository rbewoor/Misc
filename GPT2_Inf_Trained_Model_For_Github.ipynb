{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Trained model to generate text\n",
    "\n",
    "## Anaconda environment setup as follows:\n",
    "#\t*** create new environment\n",
    "#conda create -n --newENV python=3.7\n",
    "#\n",
    "#\t*** general stuff\n",
    "#conda install jupyter\n",
    "#\n",
    "#\t*** Story Generation\n",
    "#conda install numpy\n",
    "#conda install requests\n",
    "#conda install regex\n",
    "#pip install tensorflow==1.15.2\n",
    "#** not done this so far pip install toposort\n",
    "#pip install gpt-2-simple\t\t\t## Successfully installed gpt-2-simple-0.7.1 regex-2020.10.23 toposort-1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
    "from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
    "from gpt_2_simple.src.accumulate import AccumulatingOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.__version__ < '2.0.0', \"gpt-2-simple currently does not support \" \\\n",
    "    \"TensorFlow 2.0. You'll need to use a virtualenv/cloud computer which \" \\\n",
    "    \"has Tensorflow 1.X on it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "## -------------------     STORY GENERATOR STARTS                  STORY GENERATOR STARTS   -------------------\n",
    "###############################################################################################################\n",
    "def my_load_gpt2(_sess,\n",
    "              _checkpoint='latest',\n",
    "              _run_name=\"run1\",\n",
    "              _checkpoint_dir=\"checkpoint\",\n",
    "              _model_name=None,\n",
    "              _model_dir='models',\n",
    "              _multi_gpu=False):\n",
    "    \"\"\"Loads the model checkpoint or existing model into a TensorFlow session for repeated predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if _model_name:\n",
    "        checkpoint_path = os.path.join(_model_dir, _model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(_checkpoint_dir, _run_name)\n",
    "    print(f\"\\ncheckpoint_path = {checkpoint_path}\\n\")\n",
    "\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    context = tf.compat.v1.placeholder(tf.int32, [1, None])\n",
    "\n",
    "    gpus = []\n",
    "    if _multi_gpu:\n",
    "        gpus = get_available_gpus()\n",
    "\n",
    "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
    "\n",
    "    if _checkpoint=='latest':\n",
    "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
    "    else:\n",
    "        ckpt = os.path.join(checkpoint_path,_checkpoint)\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver(allow_empty=True)\n",
    "    _sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    if _model_name:\n",
    "        print(f\"\\nLoading pretrained model :: {ckpt}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nLoading checkpoint :: {ckpt}\\n\")\n",
    "    saver.restore(_sess, ckpt)\n",
    "\n",
    "def reset_session(_sess, threads=-1, server=None):\n",
    "    \"\"\"Resets the current TensorFlow session, to clear memory\n",
    "    or load another model.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    _sess.close()\n",
    "\n",
    "def gen_inference_functionality(_gpt_params):\n",
    "    _seed, _LENGTH, _TEMPERATURE, _NSAMPLES, _BATCH_SIZE, _RUN_NAME = _gpt_params\n",
    "    \n",
    "    ## set up the pre-trained GPT-2 model from checkpoint directory\n",
    "    \n",
    "    ## copy checkpoint directory from gdrive - not required as saved it locally\n",
    "    ##gpt2.copy_checkpoint_from_gdrive(run_name='run1')\n",
    "    GPT2_CHECKPOINT_DIR = r'/home/rohit/PyWDUbuntu/thesis/StyGen/checkpoint/'\n",
    "    #RUN_NAME = r\"run1\"\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    print(f\"\\nTF session started\\n\")\n",
    "    #gpt2.load_gpt2(sess, run_name='run1')\n",
    "    my_load_gpt2(\n",
    "        _sess = sess,\n",
    "        _checkpoint='latest',\n",
    "        _run_name=_RUN_NAME,\n",
    "        _checkpoint_dir=GPT2_CHECKPOINT_DIR,\n",
    "        _model_name=None,\n",
    "        _model_dir='models',\n",
    "        _multi_gpu=False)\n",
    "    \n",
    "    print(f\"\\n\\nGenerating using 'prefix' = {_seed}\\n\")\n",
    "    gpt2.generate(sess,\n",
    "                  length=_LENGTH,\n",
    "                  temperature=_TEMPERATURE,\n",
    "                  prefix=_seed,\n",
    "                  nsamples=_NSAMPLES,\n",
    "                  batch_size=_BATCH_SIZE,\n",
    "                  checkpoint_dir=GPT2_CHECKPOINT_DIR\n",
    "                 )\n",
    "    \n",
    "    reset_session(sess)\n",
    "    print(f\"\\n\\nFINISHED\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "###############################################################################################################\n",
    "## ---------------------     STORY GENERATOR ENDS                  STORY GENERATOR ENDS   ---------------------\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF session started\n",
      "\n",
      "\n",
      "checkpoint_path = /home/rohit/PyWDUbuntu/thesis/StyGen/checkpoint/Run2_File11_2_checkpoint_run2\n",
      "\n",
      "\n",
      "Loading checkpoint :: /home/rohit/PyWDUbuntu/thesis/StyGen/checkpoint/Run2_File11_2_checkpoint_run2/model-17000\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/rohit/PyWDUbuntu/thesis/StyGen/checkpoint/Run2_File11_2_checkpoint_run2/model-17000\n",
      "\n",
      "\n",
      "Generating using 'prefix' = They had a couple of beers and \n",
      "\n",
      "They had a couple of beers and iced tea.\n",
      "They were all, except Bailey, and can hardly even get the paper round.\n",
      "Poor Bailey!\n",
      "Poor Bailey!\n",
      "It was a wretched pitiful thing to be watching for him -- but he did not care.\n",
      "He was all in.\n",
      "He was a little stiff, and he could not move, only you see he could not dig.\n",
      "He did not dare to ask.\n",
      "He had been out all day.\n",
      "This Sandy George was hard to find, I suppose, when he was so drunk.\n",
      "He could be seen waving his hands and waving his arm.\n",
      "They were all laughing, and it was all the show, save the topic of Tommy Robber 's dying in the next room.\n",
      "The doctor was not known.\n",
      "Worth a thought.\n",
      "Waiting on their voices, I suppose.\n",
      "The boy was evidently trying to remember, for he seemed to bounce up from the grave and shuffle off like a snow-covered asp, snuffing and shambling.\n",
      "All the while little Joe asked his father if he could think of something big enough to do the doctor the favor.\n",
      "He had never tried him for a hand, it seemed to him.\n",
      "Unlike the other fellows, he did his duty.\n",
      "To be sick, to be restless -- Oh, dear, indeed!\n",
      "It seemed as if Tommy Robber had just launched, as he now had been entirely wont to do, unwise and un\n",
      "\n",
      "\n",
      "FINISHED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Parameters to generate the story\n",
    "## NOTE: nsamples % batch_size == 0\n",
    "LENGTH = 300\n",
    "TEMPERATURE = 0.95\n",
    "NSAMPLES = 1\n",
    "BATCH_SIZE = 1\n",
    "RUN_NAME = r\"Run2_File11_2_checkpoint_run2\"\n",
    "\n",
    "## seed value\n",
    "SEED_STRING = r'they had a couple of beers and '\n",
    "seed_string = SEED_STRING.capitalize()\n",
    "\n",
    "## check samples and batch size values are compatible\n",
    "assert (NSAMPLES % BATCH_SIZE == 0) , f\"Values for NSAMPLES and BATCH_SIZE incompatible. \" \\\n",
    "        f\"NSAMPLES={NSAMPLES} % BATCH_SIZE={BATCH_SIZE} != 0\"\n",
    "\n",
    "GPT_PARAMS = (seed_string, LENGTH, TEMPERATURE, NSAMPLES, BATCH_SIZE, RUN_NAME)\n",
    "\n",
    "gen_inference_functionality(GPT_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
