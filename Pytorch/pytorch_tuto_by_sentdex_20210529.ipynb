{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb71467a",
   "metadata": {},
   "source": [
    "# Check If PyTorch Is Using The GPU\n",
    "Feb 2020<br>\n",
    "https://chrisalbon.com/deep_learning/pytorch/basics/check_if_pytorch_is_using_gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d688d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acefcc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# How many GPUs are there?\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe6366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Which GPU Is The Current GPU?\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23abb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Is PyTorch using a GPU?\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd9547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the current GPU\n",
    "print(torch.cuda.get_device_name(device=None))\n",
    "#print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262ca8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets the cuda capability of a device.\n",
    "# device = device for which to return the device capability.\n",
    "#     This function is a no-op if this argument is a negative integer.\n",
    "#     It uses the current device, given by current_device(), if device is None (default).\n",
    "# Returns the the major and minor cuda capability of the device\n",
    "torch.cuda.get_device_capability(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce227d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce GTX 1050 Ti', major=6, minor=1, total_memory=4040MB, multi_processor_count=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get device properties\n",
    "torch.cuda.get_device_properties(device=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a68e8",
   "metadata": {},
   "source": [
    "# YouTube - series by Sentdex\n",
    "\n",
    "Introduction - Deep Learning and Neural Networks with Python and Pytorch p.1<br>\n",
    "https://www.youtube.com/watch?v=BzcBsTou0C0\n",
    "\n",
    "Data - Deep Learning and Neural Networks with Python and Pytorch p.2<br>\n",
    "https://www.youtube.com/watch?v=i2yPxY2rOzs\n",
    "\n",
    "Building our Neural Network - Deep Learning and Neural Networks with Python and Pytorch p.3<br>\n",
    "https://www.youtube.com/watch?v=ixathu7U-LQ\n",
    "\n",
    "Training Model - Deep Learning and Neural Networks with Python and Pytorch p.4<br>\n",
    "https://www.youtube.com/watch?v=9j-_dOze4IM\n",
    "\n",
    "Convnet Intro - Deep Learning and Neural Networks with Python and Pytorch p.5<br>\n",
    "https://www.youtube.com/watch?v=9aYuQmMJvjA\n",
    "\n",
    "Training Convnet - Deep Learning and Neural Networks with Python and Pytorch p.6<br>\n",
    "https://www.youtube.com/watch?v=1gQR24B3ISE\n",
    "\n",
    "On the GPU - Deep Learning and Neural Networks with Python and Pytorch p.7<br>\n",
    "https://www.youtube.com/watch?v=6gk7giKER6s\n",
    "\n",
    "Model Analysis - Deep Learning and Neural Networks with Python and Pytorch p.8<br>\n",
    "https://www.youtube.com/watch?v=UuteCccDXCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643b3ff",
   "metadata": {},
   "source": [
    "# VIRTUAL ENVIRONMENT SETUP AND PACKAGES INSTALLED\n",
    "\n",
    "python3 -m venv pv5pytorchGpu1<br>\n",
    "source ~/.venvPython/pv5pytorchGpu1/bin/activate\n",
    "\n",
    "pip3 install jupyter<br>\n",
    "pip3 install pandas<br>\n",
    "pip3 install matplotlib<br>\n",
    "pip3 install opencv-python<br>\n",
    "\n",
    "### for CPU version\n",
    "pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio==0.8.1 -f<br> https://download.pytorch.org/whl/torch_stable.html<br>\n",
    "\n",
    "### for GPU version\n",
    "pip3 install torch torchvision torchaudio\n",
    "\n",
    "### Not used so far\n",
    "pip3 install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594f649",
   "metadata": {},
   "source": [
    "# Data - Deep Learning and Neural Networks with Python and Pytorch p.2\n",
    "\n",
    "# Building our Neural Network - Deep Learning and Neural Networks with Python and Pytorch p.3\n",
    "\n",
    "# Training Model - Deep Learning and Neural Networks with Python and Pytorch p.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea6716",
   "metadata": {},
   "source": [
    "Train using MNIST data and thus designing the neural network as a classifier with 10 classes.\n",
    "MNIST data is handwritten digits (0 to 9) as black and white images of size 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f760ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b826b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MNIST', \n",
    "                      train=True, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST('/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MNIST', \n",
    "                      train=False, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306a1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f629cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching pros:\n",
    "#    data too large to fit in memory,\n",
    "#    allow stepping more often and not only at end of epoch,\n",
    "#    allows the algorithm to update the weights with more generalization properties and \n",
    "#        somewhat prevents overfitting.\n",
    "#\n",
    "# Generally used between 8 and 64, but people do go larger to speed up training.\n",
    "# Also, people use base8 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36aefb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_shape(_inlist):\n",
    "    out_shape = list()\n",
    "    x = _inlist\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            out_shape.append(len(x))\n",
    "            x = x[0]\n",
    "        except:\n",
    "            break\n",
    "    return tuple(out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4340fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(data) = <class 'list'>\n",
      "\n",
      "Shape of this list: (2, 32, 1, 28, 28)\n",
      "\n",
      "data =\n",
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 0, 0, 0, 4, 9, 5, 2, 8, 3, 6, 3, 7, 3, 5, 7, 3, 1, 5, 0, 7, 8, 6, 9,\n",
      "        6, 0, 1, 5, 4, 4, 9, 5])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(f\"type(data) = {type(data)}\") # data here is a list\n",
    "    print(f\"\\nShape of this list: {get_list_shape(data)}\")\n",
    "    print(f\"\\ndata =\\n{data}\")\n",
    "    break\n",
    "\n",
    "# len(data) = 2\n",
    "# len(data[0]) = 16\n",
    "# len(data[0][0]) = 1\n",
    "# len(data[0][0][0]) = 28\n",
    "# len(data[0][0][0][0]) = 28\n",
    "# len(data[0][0][0][0][0]) : fails with 'TypeError: len() of a 0-d tensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f543df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0], data[1]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd633ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# access first image info only\n",
    "x, y = data[0][0], data[1][0]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "271a4d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffed09",
   "metadata": {},
   "source": [
    "### check the balancing of the data - i.e. the samples have equal distribution\n",
    "\n",
    "We find the data is balanced well enought with around 10% per digit as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f2e148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = dict()\n",
    "for i in range(10):\n",
    "    counter_dict[i] = 0\n",
    "for data in trainset:\n",
    "    ys = list(data[1].detach().numpy())\n",
    "    for y in ys:\n",
    "        counter_dict[y] += 1\n",
    "        total += 1\n",
    "print(counter_dict)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e890005",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: total=5923\t%=9.87\n",
      "1: total=6742\t%=11.24\n",
      "2: total=5958\t%=9.93\n",
      "3: total=6131\t%=10.22\n",
      "4: total=5842\t%=9.74\n",
      "5: total=5421\t%=9.04\n",
      "6: total=5918\t%=9.86\n",
      "7: total=6265\t%=10.44\n",
      "8: total=5851\t%=9.75\n",
      "9: total=5949\t%=9.92\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k, v in counter_dict.items():\n",
    "    print(f\"{k}: total={v}\\t%={v/total*100:.2f}\")\n",
    "print(f\"\")\n",
    "tempdf = pd.DataFrame.from_dict(counter_dict, columns=['count'], orient='index')\n",
    "tempdf.reset_index(inplace=True)\n",
    "tempdf.rename(columns={'index': 'digit'})\n",
    "#print(tempdf)\n",
    "print(\"\")\n",
    "tempdf.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498a0ba",
   "metadata": {},
   "source": [
    "For the actual network, we want to input the 28x28 flattened image.\n",
    "Take output from 10 neurons where each neuron corresponds to the ten digits 0-9.\n",
    "Ideally for any input image, we want exactly one of these neurons to be a 1 and all the others to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91015bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbf18a",
   "metadata": {},
   "source": [
    "Using ReLu for most layers except final one. There using softmax as this is a \n",
    "multi-class (10 classes) classifiction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea322bc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # dim=1, to specify which dimension values do we want the softmaxed probabilities\n",
    "        #        to sum to 1? dim=0, would point to the batch_Size dimension.\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247378d",
   "metadata": {},
   "source": [
    "### Pass some random 28x28 pixel values to the network and see it produces an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12977217",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b531aaba",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0e33b174bbd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this fails saying:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.venvPython/pv5pytorchGpu1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-201c615feecd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv5pytorchGpu1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv5pytorchGpu1/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvPython/pv5pytorchGpu1/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)"
     ]
    }
   ],
   "source": [
    "# this fails saying:\n",
    "#     RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)\n",
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0437d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## net defined with returning : x = F.log_softmax(x, dim=1)\n",
    "\n",
    "print(X.shape)\n",
    "X = X.view(-1, 28*28)\n",
    "print(X.shape)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now network defined with returning : x = F.softmax(x, dim=1)\n",
    "## instead of with with returning : x = F.log_softmax(x, dim=1)\n",
    "## note that applying the log on the output tensor yields the same kind of result as \n",
    "## obtained if directly returning with F.log_softmax in above version\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # dim=1, to specify which dimension values do we want the softmaxed probabilities\n",
    "        #        to sum to 1? dim=0, would point to the batch_Size dimension.\n",
    "        #x = F.log_softmax(x, dim=1)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "X = X.view(-1, 28*28)\n",
    "output = net(X)\n",
    "print(output)\n",
    "print(output.sum())\n",
    "print(torch.log(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786871f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.1119)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1914f",
   "metadata": {},
   "source": [
    "Redefining the network the way it was originally with F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # dim=1, to specify which dimension values do we want the softmaxed probabilities\n",
    "        #        to sum to 1? dim=0, would point to the batch_Size dimension.\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## net defined with returning : x = F.log_softmax(x, dim=1)\n",
    "\n",
    "print(X.shape)\n",
    "X = X.view(-1, 28*28)\n",
    "print(X.shape)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cede09",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we are allowing all the parameters to be adjusted.\n",
    "## but if we were doing transfer learning, we could freeze the initial layers, and\n",
    "## thus allow only the later layers to be adjustable.\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20023f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## each of the GT output is a single scalar value i.e. the digit the image represents.\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        ## data is batch of featuresets and labels\n",
    "        X, y = data\n",
    "        print(y)\n",
    "        print(f\"\")\n",
    "        print(y[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        ## data is batch of featuresets and labels\n",
    "        X, y = data\n",
    "        \n",
    "        net.zero_grad() # 0 the grads\n",
    "        output = net(X.view(-1, 28*28)) # get prediction\n",
    "        \n",
    "        ## here the ouput is scalar, so using F.nll_loss\n",
    "        ## but suppose the output was a 1-hot vector, then MSE loss would be appropriate\n",
    "        loss = F.nll_loss(output, y) # compute the loss\n",
    "        loss.backward() # calculate the gradients via backprop\n",
    "        \n",
    "        optimizer.step() # update the parameters using the gradients just calculated\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557397dc",
   "metadata": {},
   "source": [
    "### Validate the data using the testing data\n",
    "\n",
    "But we do not want the update the weights!!!\n",
    "\n",
    "Earlier version of pytorch used net.train() and net.eval() to differentiate running in \n",
    "training vs evalution mode. But this is no longer used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aaa101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(f\"\")\n",
    "print(get_list_shape(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0])\n",
    "print()\n",
    "print(torch.argmax(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = torch.rand((10))\n",
    "print(abc)\n",
    "print()\n",
    "abc1 = ( (abc * 100) // 1 ) / 10\n",
    "print(abc1)\n",
    "print()\n",
    "abc2 = torch.round(abc * 10**2) / (10**1)\n",
    "print(abc2)\n",
    "print()\n",
    "print(torch.argmax(abc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Accuracy: {correct/total:.6f}\")\n",
    "print(f\"Accuracy: {round(correct/total, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75552ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"------------\")\n",
    "    print(torch.argmax(net(X[i].view(-1, 28*28))))\n",
    "    plt.imshow(X[i].view(28,28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f085370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c9a809",
   "metadata": {},
   "source": [
    "# Convnet Intro - Deep Learning and Neural Networks with Python and Pytorch p.5\n",
    "\n",
    "# Training Convnet - Deep Learning and Neural Networks with Python and Pytorch p.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e0e4f",
   "metadata": {},
   "source": [
    "Train using Microsoft Cats v Dogs dataset and then predict if an image has a cat or a dog.\n",
    "Note: Every image will have one or the other.<br>\n",
    "Has 12.5 images of Cats, 12.5k for Dogs.<br>\n",
    "Here simply resizing the images to a fixed size (50x50).<br>\n",
    "No augmentation like padding to make same size, or flipping, rotating, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2560ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.eye(2))\n",
    "CATS_DIR = '/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MS_Cats_v_Dogs/kagglecatsanddogs_3367a/PetImages/Cat/'\n",
    "DOGS_DIR = '/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MS_Cats_v_Dogs/kagglecatsanddogs_3367a/PetImages/Dog/'\n",
    "abc = CATS_DIR # cats\n",
    "abc2 = {CATS_DIR: 0, DOGS_DIR: 1}\n",
    "print(f\"\")\n",
    "if abc == CATS_DIR:\n",
    "    print(np.eye(2)[abc2[abc]])\n",
    "else:\n",
    "    print(np.eye(2)[abc2[abc]])\n",
    "print()\n",
    "abc = DOGS_DIR # dogs\n",
    "abc2 = {CATS_DIR: 0, DOGS_DIR: 1}\n",
    "print(f\"\")\n",
    "if abc == CATS_DIR:\n",
    "    print(np.eye(2)[abc2[abc]])\n",
    "else:\n",
    "    print(np.eye(2)[abc2[abc]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28425797",
   "metadata": {},
   "source": [
    "### Create the training data and save it as .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_NP_FILE = '/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MS_Cats_v_Dogs/training_data.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = False # Set as True\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50 # making images into 50x50\n",
    "    CATS_DIR = '/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MS_Cats_v_Dogs/kagglecatsanddogs_3367a/PetImages/Cat/'\n",
    "    DOGS_DIR = '/home/rohit/PyWDUbuntu/generic/pytorch_tutorials/MS_Cats_v_Dogs/kagglecatsanddogs_3367a/PetImages/Dog/'\n",
    "    LABELS = {CATS_DIR: 0, DOGS_DIR: 1}\n",
    "    training_data = []\n",
    "    catcount = 0\n",
    "    dogcount = 1\n",
    "    total_read_count = 0\n",
    "    read_error_count = 0\n",
    "    \n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            \n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try:\n",
    "                    path = os.path.join(label, f)\n",
    "                    # convert to grayscale as color is not relevant feature distinguish cat v dog\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                    # using 1-hot vectors instead of scalar itself\n",
    "                    # so our y is not just 0 or 1 but [0,1] for cat, [1,0] for dog\n",
    "                    self.training_data.append( [np.array(img), np.eye(2)[self.LABELS[label]]] )\n",
    "\n",
    "                    if label == self.CATS_DIR:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS_DIR:\n",
    "                        self.dogcount += 1\n",
    "                except Exception as e:\n",
    "                    self.read_error_count += 1\n",
    "                    #print(f\"Image reading error: {str(e)}\")\n",
    "                    #pass\n",
    "                self.total_read_count += 1\n",
    "        \n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(TRAINING_DATA_NP_FILE, self.training_data)\n",
    "        \n",
    "        print(f\"Cats: {self.catcount}\")\n",
    "        print(f\"Dogs: {self.dogcount}\")\n",
    "        print(f\"total_read_count: {self.total_read_count}\")\n",
    "        print(f\"read_error_count: {self.read_error_count}\")\n",
    "        print(f\"Cats %: = {(100.0 * self.catcount /(self.catcount+self.dogcount)):.2f}\")\n",
    "        print(f\"Dogs %: = {(100.0 * self.dogcount /(self.catcount+self.dogcount)):.2f}\")\n",
    "\n",
    "if REBUILD_DATA == True:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(TRAINING_DATA_NP_FILE, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6304709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1][1]) # the y i.e. the 1-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_data[1][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_data[1][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d40ebe",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "<br>\n",
    "<b>For Conv2D</b><br>\n",
    "General formula for output calculation:<br>\n",
    "output dim = <br>\n",
    "inp_size + 2*padding - dilation*(kernel-1) - 1<br>\n",
    "------------------------------------------------------------        +   1<br>\n",
    "                  stride<br>\n",
    "<br>\n",
    "But with default value dilation=1, simplifies to output dim = <br>\n",
    "inp_size + 2*padding - kernel<br>\n",
    "------------------------------------------    +    1<br>\n",
    "                  stride<br>\n",
    "<br>\n",
    "<b>For MaxPool2D</b><br>\n",
    "output size = floor(InputSize/PoolSize). NOTE: this only applies for stride=KernelSize and padding=0\n",
    "<br>\n",
    "\n",
    "#### About Conv2d of PyTorch\n",
    "\n",
    "Uses nn.conv2d for the convolutional layer<br>\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')<br>\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html<br>\n",
    "<br>\n",
    "Parameters<br>\n",
    "in_channels (int) – Number of channels in the input image<br>\n",
    "\n",
    "out_channels (int) – Number of channels produced by the convolution<br>\n",
    "\n",
    "kernel_size (int or tuple) – Size of the convolving kernel<br>\n",
    "\n",
    "stride (int or tuple, optional) – Stride of the convolution. Default: 1<br>\n",
    "\n",
    "padding (int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0<br>\n",
    "\n",
    "padding_mode (string, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'<br>\n",
    "\n",
    "dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1<br>\n",
    "\n",
    "groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1<br>\n",
    "\n",
    "bias (bool, optional) – If True, adds a learnable bias to the output. Default: True<br>\n",
    "<br>\n",
    "<b>Stride</b> controls the stride for the cross-correlation, a single number or a tuple.<br>\n",
    "<b>Padding</b> controls the amount of implicit padding on both sides for padding number of points for each dimension.<br>\n",
    "<b>Dilation</b> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link (https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what dilation does.<br>\n",
    "<b>Groups</b> controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups<br>\n",
    "<b>More about Groups</b>:<br>\n",
    "=1 means all inputs convolved to all outputs<br>\n",
    "=2, operation becomes equivalent to having two conv layers side by side, each seeing half<br>the input channels and producing half the output channels, and both subsequently concatenated<br>\n",
    "= in_channels, each input channels is convolved with its own set of filters (of size = out_channels / in_channels)<br>\n",
    "<b>Parameters kernel_size, stride, padding, dilation</b> can either be:<br>\n",
    "a single int – in which case the same value is used for the height and width dimension<br>\n",
    "a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension<br>\n",
    "<br>\n",
    "Note: when using CuDNN GPU version:<br>\n",
    "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True.<br>\n",
    "\n",
    "#### About Linear of PyTorch\n",
    "\n",
    "using nn.Linear for the fully connection layers<br>\n",
    "torch.nn.Linear(in_features, out_features, bias=True)<br>\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html<br>\n",
    "\n",
    "<b>Parameters</b><br>\n",
    "in_features – size of each input sample<br>\n",
    "\n",
    "out_features – size of each output sample<br>\n",
    "\n",
    "bias – If set to False, the layer will not learn an additive bias. Default: True<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1ea01",
   "metadata": {},
   "source": [
    "#### PyTorch version without the flatter function\n",
    "This version does not use the flatten function as it was not in PyTorch earlier.<br>\n",
    "So while moving from conv layers to fc layers, you had to code in a certain way to compute it on the fly and thus define network accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1347a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 1 as grayscale, outchannels chossing as 32, kernel=5x5\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        ## so we can't simply do like this: START\n",
    "        #self.fc1 = nn.Linear(????, 512)\n",
    "        #self.fc2 = nn.Linear(512, 2)\n",
    "        ## so we can't simply do like this: END\n",
    "        \n",
    "        # the random data to pass on the fly as workaround\n",
    "        x = torch.randn(50,50).view(-1, 1, 50, 50)\n",
    "        # variable for required shape for FC layer, to be setup on the fly\n",
    "        self._to_linear = None\n",
    "        ## during init phase: self.convs inputs dummy data and get flattened dims that\n",
    "        ##    allows correct initialisation of the self.fc1 layer\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            # x will now be (batch_size, 128, unknown_1, unknown_2)\n",
    "            # x[0].shape = (128, unknown_1, unknown_2))\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "            print(f\"x[0].shape = {x[0].shape}\")\n",
    "            print(f\"self._to_linear flattened = {self._to_linear}\")\n",
    "        return x\n",
    "    def forward(self, x):\n",
    "        # run the convolutional part\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        ## \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36908769",
   "metadata": {},
   "source": [
    "#### how to use nn.Flatten - note that the flattening is done excluding the first dim, as by default start_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "f = nn.Flatten()  #start_dim = 1 by default\n",
    "f_changed = nn.Flatten(start_dim=0)\n",
    "x = torch.randn((2, 10, 3, 4))\n",
    "print(f\"Before flatten shape : {x.shape}\")\n",
    "print(f\"After flatten shape : {f(x).shape}\")\n",
    "print(f\"{f(x).shape[1]}\")\n",
    "print()\n",
    "print(f\"After flatten_changed shape : {f_changed(x).shape}\")\n",
    "print(f\"{f_changed(x).shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4aca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PyTorch version with flatten function use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e630d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # 1 as grayscale, outchannels chossing as 32, kernel=5x5\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        ## we don't want to manually calculate the dims after conv\n",
    "        ## so we can't simply do like this: START\n",
    "        #self.fc1 = nn.Linear(????, 512)\n",
    "        #self.fc2 = nn.Linear(512, 2)\n",
    "        ## so we can't simply do like this: END\n",
    "        \n",
    "        ## pass random data thru conv layers, use nn.Flatten, then access the dim to setup\n",
    "        ##    fc1 correctly\n",
    "        x = torch.randn(50,50).view(-1, 1, 50, 50)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        print(f\"shape after maxpooling = {x[0].shape}\")\n",
    "        temp_f = nn.Flatten()\n",
    "        # variable for required shape for FC layer, to be setup on the fly\n",
    "        self._to_linear = temp_f(x).shape[1]\n",
    "        print(f\"temp_f(x).shape = {temp_f(x).shape}\")\n",
    "        print(f\"self._to_linear flattened = {self._to_linear}\")\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    def forward(self, x):\n",
    "        # run the convolutional part\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        ## flatten and run fc part\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        ## not using an activation layer in the final output here\n",
    "        ## but, one could do return F.softmax(x, dim=1) ## dim=1 due to batch in dim=0\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b99f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b402167",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1902da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0][1] # 1-hot coded vector for dog or cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea6c841a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bcc58775f798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m \u001b[0;31m# scaling to values between 0 to 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mVAL_PCT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;31m# validation size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-bcc58775f798>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m \u001b[0;31m# scaling to values between 0 to 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mVAL_PCT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;31m# validation size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0 # scaling to values between 0 to 1.0\n",
    "\n",
    "y = torch.Tensor([i[i] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1 # validation size\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda19023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e843c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba40047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e422df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e3f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b694a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bbc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ee6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
